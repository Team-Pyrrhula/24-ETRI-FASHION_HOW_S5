{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경량화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch\n",
    "import torch\n",
    "\n",
    "# built-in library\n",
    "import os\n",
    "import copy\n",
    "import argparse\n",
    "\n",
    "# custom modules\n",
    "from exp_utils import get_udevice, load_cfg\n",
    "from exp_manager import Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### input options ###\n",
    "parser = argparse.ArgumentParser(description='AI Fashion Coordinator.')\n",
    "\n",
    "parser.add_argument('--cfg_path', type=str,\n",
    "                    default='./cfgs/08_cg_architecture_with_05.yaml', # 주의) 모델 변경 시 cfg 파일 경로도 변경\n",
    "                    help=\"실험에 필요한 값들을 설정해둔 yaml 파일의 경로를 입력합니다.\")\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "### input options ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Parsed arguments>\n",
      "##### global #####\n",
      "seed: 2024\n",
      "mode: pred\n",
      "num_tasks: 6\n",
      "use_multimodal: False\n",
      "--------------------\n",
      "##### path #####\n",
      "in_file_trn_dialog: /aif/data/task1.ddata.wst.txt\n",
      "in_file_tst_dialog: /aif/data/cl_eval_task1.wst.dev\n",
      "in_file_fashion: ../aif/data/mdata.wst.txt.2023.08.23\n",
      "in_file_img_feats: /aif/data/extracted_feat.json\n",
      "model_path: ./model\n",
      "model_file: 08_cg_architecture_with_05.pt\n",
      "subWordEmb_path: ../aif/sstm_v0p5_deploy/sstm_v4p49_np_n36134_d128.dat\n",
      "--------------------\n",
      "##### data #####\n",
      "permutation_iteration: 6\n",
      "num_augmentation: 5\n",
      "corr_thres: 0.9\n",
      "mem_size: 16\n",
      "--------------------\n",
      "##### model #####\n",
      "etc: {'use_batch_norm': False, 'use_dropout': False, 'zero_prob': 0.0}\n",
      "ReqMLP: {'out_size': 300, 'req_node': '[3000,2000,1000,500]'}\n",
      "PolicyNet: {'eval_node': '[6000,3000,1000,500,200][2000]'}\n",
      "--------------------\n",
      "##### exp #####\n",
      "learning_rate: 0.0001\n",
      "max_grad_norm: 40.0\n",
      "batch_size: 100\n",
      "epochs: 10\n",
      "evaluation_iteration: 10\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# load configuration file\n",
    "cfg = load_cfg(args.cfg_path)\n",
    "\n",
    "# set pred mode\n",
    "cfg['global']['mode'] = 'pred'\n",
    "\n",
    "cfg['path']['in_file_fashion'] = '../aif/data/mdata.wst.txt.2023.08.23'\n",
    "cfg['path']['subWordEmb_path'] = '../aif/sstm_v0p5_deploy/sstm_v4p49_np_n36134_d128.dat'\n",
    "\n",
    "# check configuration value\n",
    "print('<Parsed arguments>')\n",
    "for category, value in cfg.items():\n",
    "    print(f\"##### {category} #####\")\n",
    "    for name, value in cfg[category].items():\n",
    "        print(f\"{name}: {value}\")\n",
    "\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "<Initialize subword embedding>\n",
      "loading= ../aif/sstm_v0p5_deploy/sstm_v4p49_np_n36134_d128.dat\n",
      "\n",
      "<Make metadata>\n",
      "loading fashion item metadata\n",
      "vectorizing data\n",
      "_requirement.model.0.scores.0\n",
      "_requirement.model.0.scores.1\n",
      "_requirement.model.0.scores.2\n",
      "_requirement.model.0.scores.3\n",
      "_requirement.model.0.scores.4\n",
      "_requirement.model.0.scores.5\n",
      "_requirement.model.2.scores.0\n",
      "_requirement.model.2.scores.1\n",
      "_requirement.model.2.scores.2\n",
      "_requirement.model.2.scores.3\n",
      "_requirement.model.2.scores.4\n",
      "_requirement.model.2.scores.5\n",
      "_requirement.model.4.scores.0\n",
      "_requirement.model.4.scores.1\n",
      "_requirement.model.4.scores.2\n",
      "_requirement.model.4.scores.3\n",
      "_requirement.model.4.scores.4\n",
      "_requirement.model.4.scores.5\n",
      "_requirement.model.6.scores.0\n",
      "_requirement.model.6.scores.1\n",
      "_requirement.model.6.scores.2\n",
      "_requirement.model.6.scores.3\n",
      "_requirement.model.6.scores.4\n",
      "_requirement.model.6.scores.5\n",
      "_requirement.model.8.scores.0\n",
      "_requirement.model.8.scores.1\n",
      "_requirement.model.8.scores.2\n",
      "_requirement.model.8.scores.3\n",
      "_requirement.model.8.scores.4\n",
      "_requirement.model.8.scores.5\n",
      "_policy._mlp_eval.0.scores.0\n",
      "_policy._mlp_eval.0.scores.1\n",
      "_policy._mlp_eval.0.scores.2\n",
      "_policy._mlp_eval.0.scores.3\n",
      "_policy._mlp_eval.0.scores.4\n",
      "_policy._mlp_eval.0.scores.5\n",
      "_policy._mlp_eval.2.scores.0\n",
      "_policy._mlp_eval.2.scores.1\n",
      "_policy._mlp_eval.2.scores.2\n",
      "_policy._mlp_eval.2.scores.3\n",
      "_policy._mlp_eval.2.scores.4\n",
      "_policy._mlp_eval.2.scores.5\n",
      "_policy._mlp_eval.4.scores.0\n",
      "_policy._mlp_eval.4.scores.1\n",
      "_policy._mlp_eval.4.scores.2\n",
      "_policy._mlp_eval.4.scores.3\n",
      "_policy._mlp_eval.4.scores.4\n",
      "_policy._mlp_eval.4.scores.5\n",
      "_policy._mlp_eval.6.scores.0\n",
      "_policy._mlp_eval.6.scores.1\n",
      "_policy._mlp_eval.6.scores.2\n",
      "_policy._mlp_eval.6.scores.3\n",
      "_policy._mlp_eval.6.scores.4\n",
      "_policy._mlp_eval.6.scores.5\n",
      "_policy._mlp_eval.8.scores.0\n",
      "_policy._mlp_eval.8.scores.1\n",
      "_policy._mlp_eval.8.scores.2\n",
      "_policy._mlp_eval.8.scores.3\n",
      "_policy._mlp_eval.8.scores.4\n",
      "_policy._mlp_eval.8.scores.5\n",
      "_policy._mlp_rnk.0.scores.0\n",
      "_policy._mlp_rnk.0.scores.1\n",
      "_policy._mlp_rnk.0.scores.2\n",
      "_policy._mlp_rnk.0.scores.3\n",
      "_policy._mlp_rnk.0.scores.4\n",
      "_policy._mlp_rnk.0.scores.5\n",
      "_policy._mlp_rnk.2.scores.0\n",
      "_policy._mlp_rnk.2.scores.1\n",
      "_policy._mlp_rnk.2.scores.2\n",
      "_policy._mlp_rnk.2.scores.3\n",
      "_policy._mlp_rnk.2.scores.4\n",
      "_policy._mlp_rnk.2.scores.5\n"
     ]
    }
   ],
   "source": [
    "# set model\n",
    "manager = Manager(cfg, get_udevice())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 복사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = copy.deepcopy(manager._model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경량화 이전 모델 크기 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 1396.400MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in manager._model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "\n",
    "buffer_size = 0\n",
    "for buffer in manager._model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경령화 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'score' in name:\n",
    "        model_name, module_name, model_idx, _, score_idx = name.split(\".\")\n",
    "        model_idx, score_idx = int(model_idx), int(score_idx)\n",
    "        \n",
    "        if model_name == \"_requirement\":\n",
    "            score_cp = model._requirement.model[model_idx].scores[score_idx].clone().detach().to(torch.float16)\n",
    "\n",
    "            model._requirement.model[model_idx].scores[score_idx] = score_cp\n",
    "\n",
    "        elif model_name == \"_policy\":\n",
    "            if module_name == '_mlp_eval':\n",
    "                score_cp = model._policy._mlp_eval[model_idx].scores[score_idx].clone().detach().to(torch.float16)\n",
    "\n",
    "                model._policy._mlp_eval[model_idx].scores[score_idx] = score_cp\n",
    "\n",
    "            elif module_name == '_mlp_rnk':\n",
    "                score_cp = model._policy._mlp_rnk[model_idx].scores[score_idx].clone().detach().to(torch.float16)\n",
    "\n",
    "                model._policy._mlp_rnk[model_idx].scores[score_idx] = score_cp\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경량화 이후 모델 크기 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 797.943MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경량화를 적용한 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_final = cfg['path']['model_file'].split(\".\")[0] + \"_lightweight.pt\"\n",
    "file_name_final = os.path.join(cfg['path']['model_path'], model_name_final)\n",
    "torch.save({'model': model.state_dict()}, file_name_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경량화를 적용한 모델이 정상적으로 로드되는지 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_cp = copy.deepcopy(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_cp['path']['model_file'] = cfg['path']['model_file'].split(\".\")[0] + \"_lightweight.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "<Initialize subword embedding>\n",
      "loading= ../aif/sstm_v0p5_deploy/sstm_v4p49_np_n36134_d128.dat\n",
      "\n",
      "<Make metadata>\n",
      "loading fashion item metadata\n",
      "vectorizing data\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m manager_lightweight \u001b[38;5;241m=\u001b[39m Manager(cfg_cp, get_udevice())\n",
      "File \u001b[1;32mc:\\Users\\project\\competitions\\24-ETRI-FASHION_HOW_S5\\sub_task3\\supsup\\exp_manager.py:99\u001b[0m, in \u001b[0;36mManager.__init__\u001b[1;34m(self, cfg, device, exp_name)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_idx2item, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_item2idx, \\\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_item_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta_similarities, \\\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feats \u001b[38;5;241m=\u001b[39m make_metadata(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_file_fashion, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swer, \n\u001b[0;32m     94\u001b[0m                         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordi_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta_size,\n\u001b[0;32m     95\u001b[0m                         global_cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_multimodal\u001b[39m\u001b[38;5;124m'\u001b[39m], path_cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124min_file_img_feats\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     96\u001b[0m                         feats_size)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# build model\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m SupSupMLP(data_cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmem_size\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_emb_size, model_cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReqMLP\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    100\u001b[0m                         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordi_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_rnk,\n\u001b[0;32m    101\u001b[0m                         model_cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReqMLP\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreq_node\u001b[39m\u001b[38;5;124m'\u001b[39m], model_cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPolicyNet\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_node\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    102\u001b[0m                         model_cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124metc\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_batch_norm\u001b[39m\u001b[38;5;124m'\u001b[39m], use_dropout, model_cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124metc\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzero_prob\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    103\u001b[0m                         global_cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_multimodal\u001b[39m\u001b[38;5;124m'\u001b[39m], feats_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode) \u001b[38;5;66;03m# self._mode 추가\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# check model\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n",
      "File \u001b[1;32mc:\\Users\\project\\competitions\\24-ETRI-FASHION_HOW_S5\\sub_task3\\supsup\\supermask\\base.py:32\u001b[0m, in \u001b[0;36mSupSupMLP.__init__\u001b[1;34m(self, mem_size, emb_size, out_size, meta_size, coordi_size, num_rnk, req_node, eval_node, use_batch_norm, use_dropout, zero_prob, use_multimodal, img_feat_size, num_tasks, mode)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# train, test, eval\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# class instance for requirement estimation\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requirement \u001b[38;5;241m=\u001b[39m RequirementMLP(mem_size, emb_size, req_node,\n\u001b[0;32m     33\u001b[0m                                    use_dropout, zero_prob,\n\u001b[0;32m     34\u001b[0m                                    use_batch_norm, out_size,\n\u001b[0;32m     35\u001b[0m                                    num_tasks, mode) \u001b[38;5;66;03m# mode 추가\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# class instance for ranking\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_policy \u001b[38;5;241m=\u001b[39m PolicyNet(emb_size, out_size, meta_size, coordi_size,\n\u001b[0;32m     39\u001b[0m                          num_rnk, eval_node, use_batch_norm,\n\u001b[0;32m     40\u001b[0m                          use_dropout, zero_prob, use_multimodal,\n\u001b[0;32m     41\u001b[0m                          img_feat_size, num_tasks, mode)\n",
      "File \u001b[1;32mc:\\Users\\project\\competitions\\24-ETRI-FASHION_HOW_S5\\sub_task3\\supsup\\supermask\\requirement.py:72\u001b[0m, in \u001b[0;36mRequirementMLP.__init__\u001b[1;34m(self, mem_size, emb_size, architecture, use_dropout, zero_prob, use_batchnorm, out_size, num_tasks, mode)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     num_out \u001b[38;5;241m=\u001b[39m architecture[i]\n\u001b[0;32m     71\u001b[0m     model\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m---> 72\u001b[0m         MultitaskMaskLinear(\n\u001b[0;32m     73\u001b[0m             num_in,\n\u001b[0;32m     74\u001b[0m             num_out,\n\u001b[0;32m     75\u001b[0m             mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m     76\u001b[0m             num_tasks\u001b[38;5;241m=\u001b[39mnum_tasks,\n\u001b[0;32m     77\u001b[0m             bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     78\u001b[0m         )\n\u001b[0;32m     79\u001b[0m     )\n\u001b[0;32m     80\u001b[0m     model\u001b[38;5;241m.\u001b[39mappend(nn\u001b[38;5;241m.\u001b[39mReLU())\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_dropout:\n",
      "File \u001b[1;32mc:\\Users\\project\\competitions\\24-ETRI-FASHION_HOW_S5\\sub_task3\\supsup\\supermask\\linear.py:22\u001b[0m, in \u001b[0;36mMultitaskMaskLinear.__init__\u001b[1;34m(self, mode, num_tasks, *args, **kwargs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# task별로 layer의 score를 따로 기록\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# for lightweighting model prediction\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscores \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameterList([\n\u001b[1;32m---> 22\u001b[0m         nn\u001b[38;5;241m.\u001b[39mParameter(mask_init(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat16)) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_tasks)\n\u001b[0;32m     23\u001b[0m     ]) \u001b[38;5;66;03m# TODO: dtype 인자로 받기\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscores \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameterList([\n\u001b[0;32m     27\u001b[0m         nn\u001b[38;5;241m.\u001b[39mParameter(mask_init(\u001b[38;5;28mself\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_tasks)\n\u001b[0;32m     28\u001b[0m     ])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "manager_lightweight = Manager(cfg_cp, get_udevice())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_requirement.model.0.scores.0 torch.float32\n",
      "_requirement.model.0.scores.1 torch.float32\n",
      "_requirement.model.0.scores.2 torch.float32\n",
      "_requirement.model.0.scores.3 torch.float32\n",
      "_requirement.model.0.scores.4 torch.float32\n",
      "_requirement.model.0.scores.5 torch.float32\n",
      "_requirement.model.2.scores.0 torch.float32\n",
      "_requirement.model.2.scores.1 torch.float32\n",
      "_requirement.model.2.scores.2 torch.float32\n",
      "_requirement.model.2.scores.3 torch.float32\n",
      "_requirement.model.2.scores.4 torch.float32\n",
      "_requirement.model.2.scores.5 torch.float32\n",
      "_requirement.model.4.scores.0 torch.float32\n",
      "_requirement.model.4.scores.1 torch.float32\n",
      "_requirement.model.4.scores.2 torch.float32\n",
      "_requirement.model.4.scores.3 torch.float32\n",
      "_requirement.model.4.scores.4 torch.float32\n",
      "_requirement.model.4.scores.5 torch.float32\n",
      "_requirement.model.6.scores.0 torch.float32\n",
      "_requirement.model.6.scores.1 torch.float32\n",
      "_requirement.model.6.scores.2 torch.float32\n",
      "_requirement.model.6.scores.3 torch.float32\n",
      "_requirement.model.6.scores.4 torch.float32\n",
      "_requirement.model.6.scores.5 torch.float32\n",
      "_requirement.model.8.scores.0 torch.float32\n",
      "_requirement.model.8.scores.1 torch.float32\n",
      "_requirement.model.8.scores.2 torch.float32\n",
      "_requirement.model.8.scores.3 torch.float32\n",
      "_requirement.model.8.scores.4 torch.float32\n",
      "_requirement.model.8.scores.5 torch.float32\n",
      "_policy._mlp_eval.0.scores.0 torch.float32\n",
      "_policy._mlp_eval.0.scores.1 torch.float32\n",
      "_policy._mlp_eval.0.scores.2 torch.float32\n",
      "_policy._mlp_eval.0.scores.3 torch.float32\n",
      "_policy._mlp_eval.0.scores.4 torch.float32\n",
      "_policy._mlp_eval.0.scores.5 torch.float32\n",
      "_policy._mlp_eval.2.scores.0 torch.float32\n",
      "_policy._mlp_eval.2.scores.1 torch.float32\n",
      "_policy._mlp_eval.2.scores.2 torch.float32\n",
      "_policy._mlp_eval.2.scores.3 torch.float32\n",
      "_policy._mlp_eval.2.scores.4 torch.float32\n",
      "_policy._mlp_eval.2.scores.5 torch.float32\n",
      "_policy._mlp_eval.4.scores.0 torch.float32\n",
      "_policy._mlp_eval.4.scores.1 torch.float32\n",
      "_policy._mlp_eval.4.scores.2 torch.float32\n",
      "_policy._mlp_eval.4.scores.3 torch.float32\n",
      "_policy._mlp_eval.4.scores.4 torch.float32\n",
      "_policy._mlp_eval.4.scores.5 torch.float32\n",
      "_policy._mlp_eval.6.scores.0 torch.float32\n",
      "_policy._mlp_eval.6.scores.1 torch.float32\n",
      "_policy._mlp_eval.6.scores.2 torch.float32\n",
      "_policy._mlp_eval.6.scores.3 torch.float32\n",
      "_policy._mlp_eval.6.scores.4 torch.float32\n",
      "_policy._mlp_eval.6.scores.5 torch.float32\n",
      "_policy._mlp_eval.8.scores.0 torch.float32\n",
      "_policy._mlp_eval.8.scores.1 torch.float32\n",
      "_policy._mlp_eval.8.scores.2 torch.float32\n",
      "_policy._mlp_eval.8.scores.3 torch.float32\n",
      "_policy._mlp_eval.8.scores.4 torch.float32\n",
      "_policy._mlp_eval.8.scores.5 torch.float32\n",
      "_policy._mlp_rnk.0.scores.0 torch.float32\n",
      "_policy._mlp_rnk.0.scores.1 torch.float32\n",
      "_policy._mlp_rnk.0.scores.2 torch.float32\n",
      "_policy._mlp_rnk.0.scores.3 torch.float32\n",
      "_policy._mlp_rnk.0.scores.4 torch.float32\n",
      "_policy._mlp_rnk.0.scores.5 torch.float32\n",
      "_policy._mlp_rnk.2.scores.0 torch.float32\n",
      "_policy._mlp_rnk.2.scores.1 torch.float32\n",
      "_policy._mlp_rnk.2.scores.2 torch.float32\n",
      "_policy._mlp_rnk.2.scores.3 torch.float32\n",
      "_policy._mlp_rnk.2.scores.4 torch.float32\n",
      "_policy._mlp_rnk.2.scores.5 torch.float32\n"
     ]
    }
   ],
   "source": [
    "for name, param in manager_lightweight._model.named_parameters():\n",
    "    if 'score' in name:\n",
    "        print(name, param.dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lgaimers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
